{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Missing links in a citation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jgraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ed4fa281a0a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mjgraph\u001b[0m \u001b[1;31m## this was previously known as igraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jgraph'"
     ]
    }
   ],
   "source": [
    "# global imports \n",
    "import random \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import jgraph ## this was previously known as igraph\n",
    "import csv \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning imports\n",
    "from sklearn import svm \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel \n",
    "from sklearn import preprocessing \n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read data from txt files\n",
    "nodes_info_df = pd.read_csv('./data/node_information.csv')\n",
    "random_preds_df = pd.read_csv('./data/random_predictions.csv') \n",
    "test_set = pd.read_csv('./data/testing_set.txt', sep = ' ', header = None)\n",
    "train_set = pd.read_csv('./data/training_set.txt', sep = ' ', header = None)\n",
    "test_set.columns = ['source_id', 'target_id']\n",
    "train_set.columns = ['source_id', 'target_id', 'label']\n",
    "nodes_info_df.columns = ['paper_id', 'publication_year', 'title', 'author', 'journal_name', 'abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique papers: ', len(set(nodes_info_df['paper_id'])))\n",
    "sym_diff = set(test_set['source_id'].append(test_set['target_id'])).symmetric_difference(set(nodes_info_df['paper_id']))\n",
    "print('Unknown papers in test set (with nodes_info):', len(sym_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get distribution of journal names \n",
    "# nodes_info_df['journal_name'] = nodes_info_df['journal_name'].fillna('unknown')\n",
    "# nodes_info_df.journal_name.value_counts()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes_info_df.author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Spacy\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load(disable=[\"tagger\", \"parser\",\"ner\",\"entity_linker\",\"textcat\",\"entity_ruler\",\"sentencizer\",\"merge_noun_chunks\",\"merge_entities\",\"merge_subtokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "\n",
    "def filter_bad(alphabet):\n",
    "    bad = [',', None]\n",
    "\n",
    "    if(alphabet in bad):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "## possible formats of authors:\n",
    "# several authors: separation via ','\n",
    "# sometimes mentions the university eg '(montpellier)'\n",
    "# sometimes mentions the first name \n",
    "# sometimes format is: firstname letter. lastname\n",
    "def author_normalisation(authors):\n",
    "    if isNaN(authors) == False:\n",
    "        #print(authors)\n",
    "        authors = authors.lower()\n",
    "        final_authors = list()\n",
    "        \n",
    "        # remove universities and last space\n",
    "        if '(' in authors:\n",
    "            authors = re.sub(r'\\(+.*\\)', '', authors).strip() \n",
    "        \n",
    "        # remove extra spaces\n",
    "        authors = authors.split()\n",
    "        authors = ' '.join(filter(filter_bad, authors))\n",
    "          \n",
    "        # get all authors of one paper \n",
    "        for author in authors.split(', '): \n",
    "            author.strip()            \n",
    "            # get the names of an author\n",
    "            names = author.split(' ')\n",
    "            author_names = list()        \n",
    "            if len(names) == 2:\n",
    "                # check if first element is 'letter.' format:\n",
    "                if re.match('\\w\\.', names[0]):\n",
    "                    author_names.append(names[0])\n",
    "                else:\n",
    "                    author_names.append(names[0][0] + '.')\n",
    "\n",
    "            if len(names) == 3:\n",
    "                if re.match('\\w\\.', names[0]):\n",
    "                    author_names.append(names[0])\n",
    "                else:\n",
    "                    author_names.append(names[0][0] + '.')\n",
    "\n",
    "                # skip the second middle name\n",
    "                if re.match('\\w\\.', names[1]):\n",
    "                    pass\n",
    "                    #author_names.append(names[1])\n",
    "                #else:\n",
    "                #    author_names.append(names[1][0] + '.')\n",
    "\n",
    "            author_names.append(names[-1])\n",
    "            if len(author_names) > 1:\n",
    "                author_names = ' '.join(author_names)\n",
    "            else:\n",
    "                author_names = author_names[0]\n",
    "            # append last name\n",
    "            final_authors.append(author_names)\n",
    "\n",
    "\n",
    "        number_of_authors = len(final_authors)\n",
    "        if number_of_authors == 0:\n",
    "            return np.NaN\n",
    "        return final_authors\n",
    "    \n",
    "    return np.NaN\n",
    "\n",
    "def common_authors(string1, string2):\n",
    "    if isNaN(string1):\n",
    "        return False\n",
    "    if isNaN(string2):\n",
    "        return False\n",
    "    \n",
    "    #a_set = set(string1.split(','))\n",
    "    #b_set = set(string2.split(','))\n",
    "    a_set = set(string1)\n",
    "    b_set = set(string2)\n",
    "    \n",
    "    if (a_set & b_set): \n",
    "        return True \n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def number_common_authors(string1, string2):\n",
    "    pass\n",
    "\n",
    "\n",
    "def remove_special_characters(string):\n",
    "    string = re.sub(\"([^\\w]|[\\d_])+\", \" \",  string)\n",
    "    return string\n",
    "\n",
    "def tokenize(string):        \n",
    "    # Code to tokenize\n",
    "    spacy_tokens = spacy_nlp(string)\n",
    "    # Code to remove punctuation tokens and create string tokens\n",
    "    string_tokens = [token.lemma_ for token in spacy_tokens if not token.is_punct if not token.is_stop]  \n",
    "    return string_tokens      \n",
    "\n",
    "def recombining_tokens_into_a_string(list_of_tokens):\n",
    "    return \" \".join(list_of_tokens)\n",
    "\n",
    "\n",
    "def create_tf_idf(column,tf_idf):\n",
    "    #if tf_idf doesn't exist\n",
    "    if tf_idf==None:\n",
    "        #create a TfidfVectorizer object\n",
    "        tf_idf = TfidfVectorizer()\n",
    "        #Vectorize the sample text\n",
    "        X_tfidf_sample = tf_idf.fit_transform(column)\n",
    "    #if tf_idf already exist use the same for the test\n",
    "    else:\n",
    "        X_tfidf_sample = tf_idf.transform(column)\n",
    "    return X_tfidf_sample,tf_idf\n",
    "\n",
    "def tf_idf_feature(column,dataset,tf_idf,author_or_not):\n",
    "    #Remove special characters from the text\n",
    "    dataset[column]=dataset[column].apply(lambda x: remove_special_characters(x))\n",
    "    #if we deal with the column author\n",
    "    if author_or_not==1:\n",
    "        # Remove strings of size less than two\n",
    "        column_cleaned= dataset[column].str.findall('\\w{2,}').str.join(' ')\n",
    "    else:\n",
    "        #Tokenize, extract lemmas and remove stop words\n",
    "        tokenized=dataset[column].apply(lambda x: tokenize(x)) \n",
    "        #Recombine tokens into a string\n",
    "        column_cleaned=tokenized.apply(recombining_tokens_into_a_string)\n",
    "    # Create the tf_idf matrix \n",
    "    tf_idf_matrix,tf_idf=create_tf_idf(column_cleaned,tf_idf)\n",
    "    return tf_idf_matrix,tf_idf\n",
    "\n",
    "# Compute the similarity between a column target and source\n",
    "def compute_similarity(column,df_source,df_target,author_or_not):\n",
    "    #Fill the Na's\n",
    "    df_source[column].fillna(\"unknown\", inplace=True)\n",
    "    df_target[column].fillna(\"unknown\", inplace=True)\n",
    "    tf_idf=None\n",
    "    #Create the tf_idf features\n",
    "    tf_idf_title_source,tf_idf=tf_idf_feature(column,df_source,tf_idf,author_or_not)\n",
    "    tf_idf_title_target,tf_idf=tf_idf_feature(column,df_target,tf_idf,author_or_not)\n",
    "    #Calculate the similarities\n",
    "    similarity=[]\n",
    "    for i in range(tf_idf_title_source.shape[0]):\n",
    "        cos_sim=cosine_similarity(tf_idf_title_source[i], tf_idf_title_target[i])\n",
    "        similarity.append(cos_sim)\n",
    "    #Convert the list as a DataFrame\n",
    "    similarity_df=pd.DataFrame(np.vstack(similarity))\n",
    "    return similarity_df\n",
    "\n",
    "def reduce_matrix_width(source_df,target_df,n_components):\n",
    "    # Apply a PCA to reduce the matrix width , we chose 15\n",
    "    pca_train = PCA(n_components=n_components)\n",
    "    #PCA on source feature\n",
    "    pca_train.fit(source_df)\n",
    "    matrix_source_reduced = pca_train.transform(source_df)\n",
    "    print(sum(pca_train.explained_variance_ratio_)) # Percentage of initial matrix explained by reduced matrix\n",
    "    #PCA on target feature\n",
    "    pca_train.fit(target_df)\n",
    "    matrix_target_reduced = pca_train.transform(target_df)\n",
    "    print(sum(pca_train.explained_variance_ratio_)) # Percentage of initial matrix explained by reduced matrix\n",
    "    return matrix_source_reduced,matrix_target_reduced\n",
    "\n",
    "def journal_name_feature():\n",
    "    #We first merge train and test to avoid a different number of features when one-hot-encoding\n",
    "    #To keep trace of the train and test dataset\n",
    "    train_source_info['train_test']=1\n",
    "    train_target_info['train_test']=1\n",
    "    test_source_info['train_test']=0\n",
    "    test_target_info['train_test']=0\n",
    "    # merging the two datasets together\n",
    "    combined_source=pd.concat([train_source_info,test_source_info],ignore_index=True)\n",
    "    combined_target=pd.concat([train_target_info,test_target_info],ignore_index=True)\n",
    "    # One hot encoding\n",
    "    journal_name_encoded_source=pd.get_dummies(combined_source['journal_name'])\n",
    "    journal_name_encoded_target=pd.get_dummies(combined_target['journal_name'])\n",
    "    #Apply PCA to reduce matrix with 15 components\n",
    "    journal_name_encoded_source_reduced,journal_name_encoded_target_reduced =reduce_matrix_width(journal_name_encoded_source,journal_name_encoded_target,15)\n",
    "    # Merge encoded dataset with the combine dataset\n",
    "    combined_source=pd.concat([combined_source,pd.DataFrame(journal_name_encoded_source_reduced)],axis=1)\n",
    "    combined_target=pd.concat([combined_target,pd.DataFrame(journal_name_encoded_target_reduced)],axis=1)\n",
    "    #Separate train and test and keep only journal_name features\n",
    "    train_source_journal=combined_source[combined_source[\"train_test\"]==1].drop(['abstract','author','journal_name','label','paper_id','publication_year','source_id','target_id','title','train_test'], axis=1)\n",
    "    test_source_journal=combined_source[combined_source[\"train_test\"]==0].drop(['abstract','author','journal_name','label','paper_id','publication_year','source_id','target_id','title','train_test'], axis=1)\n",
    "    train_target_journal=combined_target[combined_target[\"train_test\"]==1].drop(['abstract','author','journal_name','label','paper_id','publication_year','source_id','target_id','title','train_test'], axis=1)\n",
    "    test_target_journal=combined_target[combined_target[\"train_test\"]==0].drop(['abstract','author','journal_name','label','paper_id','publication_year','source_id','target_id','title','train_test'], axis=1)\n",
    "    #add prefix to columns names\n",
    "    train_source_journal.columns=[str(col) + '_source' for col in train_source_journal.columns]\n",
    "    test_source_journal.columns=[str(col) + '_source' for col in test_source_journal.columns]\n",
    "    train_target_journal.columns=[str(col) + '_target' for col in train_target_journal.columns]\n",
    "    test_target_journal.columns=[str(col) + '_target' for col in test_target_journal.columns]\n",
    "    return train_source_journal,test_source_journal,train_target_journal,test_target_journal\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reaye source and target info datasets\n",
    "train_source_info = train_set.merge(nodes_info_df, left_on='source_id', right_on='paper_id',how=\"left\")\n",
    "train_target_info = train_set.merge(nodes_info_df, left_on='target_id', right_on='paper_id',how=\"left\")\n",
    "\n",
    "test_source_info = test_set.merge(nodes_info_df, left_on='source_id', right_on='paper_id',how=\"left\")\n",
    "test_target_info = test_set.merge(nodes_info_df, left_on='target_id', right_on='paper_id',how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## apply the features to training set \n",
    "train_set['source_authors'] = train_source_info.author.apply(lambda x: author_normalisation(x))\n",
    "train_set['target_authors'] = train_target_info.author.apply(lambda x: author_normalisation(x))\n",
    "\n",
    "train_set['publication_year_diff'] = train_source_info.publication_year - train_target_info.publication_year\n",
    "\n",
    "train_set['source_journal'] = train_source_info.journal_name\n",
    "train_set['target_journal'] = train_target_info.journal_name\n",
    "\n",
    "train_set['same_journal'] = train_set.apply(lambda x: int(x.source_journal == x.target_journal), axis=1)\n",
    "\n",
    "## apply the features to test set\n",
    "test_set['source_authors'] = test_source_info.author.apply(lambda x: author_normalisation(x))\n",
    "test_set['target_authors'] = test_target_info.author.apply(lambda x: author_normalisation(x))\n",
    "\n",
    "test_set['publication_year_diff'] = test_source_info.publication_year - test_target_info.publication_year\n",
    "\n",
    "test_set['source_journal'] = test_source_info.journal_name\n",
    "test_set['target_journal'] = test_target_info.journal_name\n",
    "test_set['same_journal'] = test_set.apply(lambda x: int(x.source_journal == x.target_journal), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other features this might take some times to run\n",
    "## apply the features to training set\n",
    "train_set['similarity_title']=compute_similarity(\"title\",train_source_info,train_target_info,0)\n",
    "train_set['similarity_abstract']=compute_similarity(\"abstract\",train_source_info,train_target_info,0)\n",
    "train_set['similarity_author']=compute_similarity(\"author\",train_source_info,train_target_info,1)\n",
    "\n",
    "## apply features to test set\n",
    "test_set['similarity_title']=compute_similarity(\"title\",test_source_info,test_target_info,0)\n",
    "test_set['similarity_abstract']=compute_similarity(\"abstract\",test_source_info,test_target_info,0)\n",
    "test_set['similarity_author']=compute_similarity(\"author\",test_source_info,test_target_info,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#journal_name feature\n",
    "train_source_journal,test_source_journal,train_target_journal,test_target_journal =journal_name_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add journal_name to the train and test\n",
    "train_set=pd.concat([train_set,train_source_journal],axis=1,)\n",
    "train_set=pd.concat([train_set,train_target_journal],axis=1)\n",
    "test_set=pd.concat([test_set,test_source_journal.reset_index().drop([\"index\"],axis=1)],axis=1)\n",
    "test_set=pd.concat([test_set,test_target_journal.reset_index().drop([\"index\"],axis=1)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph features generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "# get some elements and then assign the attributes -> this is shite so ignore it \n",
    "def shortest_path_info(some_graph, source, target):\n",
    "    if source not in some_graph.nodes():\n",
    "        return -1 # not known \n",
    "    if target not in some_graph.nodes():\n",
    "        return -1 # not known \n",
    "    if nx.has_path(some_graph, source, target):\n",
    "        return nx.dijkstra_path_length(some_graph, source=source, target=target)\n",
    "    \n",
    "    return -2 # no path\n",
    "\n",
    "def degree_centrality(some_graph):\n",
    "    degree_dict = dict(some_graph.degree(some_graph.nodes()))\n",
    "    return degree_dict\n",
    "\n",
    "def get_in_out_degree(some_graph):\n",
    "    in_degree_dict = dict(some_graph.in_degree(some_graph.nodes()))\n",
    "    out_degree_dict = dict(some_graph.out_degree(some_graph.nodes()))\n",
    "    return in_degree_dict, out_degree_dict\n",
    "    \n",
    "\n",
    "def common_neighs(some_graph, x, y):\n",
    "    if x not in some_graph.nodes():\n",
    "        return 0,[] # not known \n",
    "    if y not in some_graph.nodes():\n",
    "        return 0,[] # not known\n",
    "    neighs = sorted(list(nx.common_neighbors(some_graph, x, y)))\n",
    "    return len(neighs), neighs\n",
    "\n",
    "def jac_index(g, x, y):\n",
    "    if x not in g.nodes():\n",
    "        return -1 # not known \n",
    "    if y not in g.nodes():\n",
    "        return -1 # not known\n",
    "    preds = nx.jaccard_coefficient(g, [(x, y)])\n",
    "    jacc = 0\n",
    "\n",
    "    for u, v, p in preds:\n",
    "        jacc = p\n",
    "    return jacc\n",
    "\n",
    "def pref_attachement(g, x, y):\n",
    "    if x not in g.nodes():\n",
    "        return -1 # not known \n",
    "    if y not in g.nodes():\n",
    "        return -1 # not known\n",
    "    preds = nx.preferential_attachment(g, [(x, y)])\n",
    "    pref = 0\n",
    "\n",
    "    for u, v, p in preds:\n",
    "        pref = p\n",
    "    return pref\n",
    "\n",
    "def aa_index(g, x, y):\n",
    "    if x not in g.nodes():\n",
    "        return -1 # not known \n",
    "    if y not in g.nodes():\n",
    "        return -1 # not known\n",
    "    preds = nx.adamic_adar_index(g, [(x, y)])\n",
    "    aa = 0\n",
    "\n",
    "    for u, v, p in preds:\n",
    "        aa = p\n",
    "    return aa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network \n",
    "# get network for when there is a connection in train set\n",
    "# edges = list(zip(train_set.loc[train_set.label == 1].source_id, train_set.loc[train_set.label == 1].target_id))\n",
    "# nodes = list(set(train_set.source_id + train_set.target_id))\n",
    "\n",
    "# train_G = nx.DiGraph()\n",
    "# train_G.add_nodes_from(nodes)\n",
    "# train_G.add_edges_from(edges)\n",
    "\n",
    "train_G = nx.from_pandas_edgelist(train_set, source='source_id', target='target_id', edge_attr=None,\n",
    "                                  create_using=nx.DiGraph())\n",
    "\n",
    "# make sure you also have an undirected graph\n",
    "train_G_ud = train_G.to_undirected()\n",
    "\n",
    "# create some dictionaries to use later on\n",
    "clustering_coeff_dict = nx.clustering(train_G_ud)\n",
    "avg_neigh_degree_dict = nx.average_neighbor_degree(train_G)\n",
    "out_degree_centrality = nx.out_degree_centrality(train_G)\n",
    "in_degree_centrality = nx.in_degree_centrality(train_G)\n",
    "page_rank = nx.pagerank_scipy(train_G)\n",
    "hub_score, authority_score = nx.hits(train_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get features for graph of a single element\n",
    "def get_features(directed_graph, ud_graph, source_id, target_id, label):\n",
    "    # features for undirected graph\n",
    "    jaccard_index = jac_index(ud_graph, source_id, target_id)\n",
    "    preferencial_attachment = pref_attachement(ud_graph, source_id, target_id)\n",
    "    number_common_neighbours, common_neighbours = common_neighs(ud_graph, source_id, target_id)\n",
    "    adamic_adar_index = aa_index(ud_graph, source_id, target_id)\n",
    "    #shortest_path = shortest_path_info(train_G, source_id, target_id)\n",
    "\n",
    "    \n",
    "    source_pr = page_rank[source_id]\n",
    "    source_hub_score = hub_score[source_id]\n",
    "    source_authority_score = authority_score[source_id]\n",
    "    source_cluster_coeff = clustering_coeff_dict[source_id]\n",
    "    source_out_centrality = out_degree_centrality[source_id]\n",
    "    source_avg_neigh_degree = avg_neigh_degree_dict[source_id]\n",
    " \n",
    "    target_pr = page_rank[target_id]\n",
    "    target_hub_score = hub_score[target_id]\n",
    "    target_authority_score = authority_score[target_id]\n",
    "    target_cluster_coeff = clustering_coeff_dict[target_id]\n",
    "    target_in_centrality = in_degree_centrality[target_id]\n",
    "    target_avg_neigh_degree = avg_neigh_degree_dict[target_id]\n",
    "\n",
    "    # no name feature but supposedly important \n",
    "    feature_n = source_out_centrality * target_in_centrality\n",
    "     \n",
    "    return [source_id, target_id, label, jaccard_index, preferencial_attachment, \n",
    "            number_common_neighbours, adamic_adar_index, source_pr, target_pr, \n",
    "            source_hub_score, target_hub_score, source_authority_score, \n",
    "            target_authority_score, source_cluster_coeff, target_cluster_coeff, \n",
    "            source_out_centrality, target_in_centrality, source_avg_neigh_degree, \n",
    "            target_avg_neigh_degree, feature_n]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT: add column names when adding new features to the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add columns when you add Features\n",
    "column_names = ['source_id', 'target_id', 'label', 'jaccard_index', 'preferential_attachement', \n",
    "                'number_common_neighbours',  'adamic_adar_index', 'source_pr',\n",
    "                'target_pr', 'source_hub_score', 'target_hub_score', 'source_authority_score',\n",
    "                'target_authority_score', 'source_cluster_coeff', 'target_cluster_coeff',\n",
    "                'source_out_centrality', 'target_in_centrality', 'source_avg_neigh_degree', \n",
    "                'target_avg_neigh_degree', 'feature_n']\n",
    "final_train_set = pd.DataFrame([[np.nan]*len(column_names)]* train_set.shape[0], columns=column_names)\n",
    "final_test_set = pd.DataFrame([[np.nan]*len(column_names)]* test_set.shape[0], columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the features for the train set\n",
    "for idx, row in train_set.iterrows():\n",
    "    features = get_features(train_G, train_G_ud, row.source_id, row.target_id, row.label)\n",
    "    #update the features\n",
    "    final_train_set.loc[idx] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the features for the test set\n",
    "for idx, row in test_set.iterrows():\n",
    "    features = get_features(train_G, train_G_ud, row.source_id, row.target_id, -1)\n",
    "    #update the features\n",
    "    final_test_set.loc[idx] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge graph and text features together \n",
    "train_set = train_set.merge(final_train_set, on=['source_id', 'target_id', 'label'], how='left') \n",
    "test_set = test_set.merge(final_test_set, on=['source_id', 'target_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import betweenness_centrality\n",
    "from networkx import edge_betweenness_centrality\n",
    "from networkx import load_centrality\n",
    "from networkx import eigenvector_centrality\n",
    "\n",
    "def graph_features(directed_graph, dataframe_dataset):\n",
    "    # betweenness\n",
    "    between_centrality = betweenness_centrality(directed_graph) # shortest-path betweenness centrality for nodes\n",
    "    # load centrality\n",
    "    ld_centrality = load_centrality(directed_graph) # load centrality of a node is the fraction of all shortest paths that pass through that node\n",
    "    #eigenvector centrality\n",
    "    eig_centrality = eigenvector_centrality(directed_graph)\n",
    "    \n",
    "    # save features to training set \n",
    "    dataframe_dataset['betweeness_centrality'] = pd.DataFrame.from_dict(dict(eig_centrality), orient='index')\n",
    "    dataframe_dataset['load_centrality'] = pd.DataFrame.from_dict(dict(ld_centrality), orient='index')\n",
    "    dataframe_dataset['eigen_centrality'] = pd.DataFrame.from_dict(dict(eig_centrality), orient='index')\n",
    "\n",
    "    return dataframe_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = graph_features(train_G, train_set)\n",
    "train_set.betweeness_centrality.fillna(-1, inplace=True)\n",
    "train_set.load_centrality.fillna(-1, inplace=True)\n",
    "train_set.eigen_centrality.fillna(-1, inplace=True)\n",
    "\n",
    "test_set = graph_features(train_G, test_set)\n",
    "test_set.betweeness_centrality.fillna(-1, inplace=True)\n",
    "test_set.load_centrality.fillna(-1, inplace=True)\n",
    "test_set.eigen_centrality.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out so that you do not have to run everything again\n",
    "train_set.to_csv('final_train.csv',index=False)\n",
    "test_set.to_csv('final_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can start from here as well when features were saved previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('final_test.csv')\n",
    "train_set = pd.read_csv('final_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final clean (i.e replacing nans etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nas in some way\n",
    "train_set.publication_year_diff.fillna(-24, inplace=True) # 24 is for unknown (?)\n",
    "train_set.fillna('unknown', inplace=True)\n",
    "\n",
    "test_set.publication_year_diff.fillna(-24, inplace=True) # 24 is for unknown (?_)\n",
    "test_set.fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the types of each column (none should be object)\n",
    "train_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "## Most interesting correlation is with label\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(train_set.corr(),\n",
    "            vmax=0.5,\n",
    "            square=True,\n",
    "            annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from labels:\n",
    "X = train_set.loc[:, (train_set.columns != 'label') & \n",
    "                  (train_set.columns != 'common_authors') & \n",
    "                  (train_set.columns != 'source_authors') & \n",
    "                  (train_set.columns != 'target_authors') & \n",
    "                  (train_set.columns != 'source_journal') & \n",
    "                  (train_set.columns != 'target_journal') \n",
    "                 ]\n",
    "y = train_set['label']\n",
    "y.astype(np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final feature correlation\n",
    "ff = X.copy()\n",
    "ff['label'] = y\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(X.corr(),\n",
    "            vmax=0.5,\n",
    "            square=True,\n",
    "            annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train different models and compare the performance \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import  f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=75, learning_rate=1)\n",
    "scores = cross_validate(model, X, y, scoring='f1', \n",
    "                        cv=5) # n_jobs is the number of cpus to use -1 => all\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe results from scores\n",
    "from scipy import stats \n",
    "stats.describe(scores['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "scores = cross_validate(model, X, y, scoring='f1', \n",
    "                        cv=5) # n_jobs is the number of cpus to use -1 => all\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe results from scores\n",
    "from scipy import stats \n",
    "stats.describe(scores['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ONLY RUN AT THE END FOR GRAPHS.. takes a v.long time to execute (been 3hours for now.. only execute on a virtual \n",
    "# # machine with GPUs (if possible))\n",
    "# from sklearn.feature_selection import RFECV\n",
    "\n",
    "# clf_rf_4 = model\n",
    "# rfecv = RFECV(estimator=clf_rf_4, step=1, cv=10,scoring='f1')   #10-fold cross-validation\n",
    "# rfecv = rfecv.fit(X, y)\n",
    "\n",
    "# print('Optimal number of features :', rfecv.n_features_)\n",
    "# print('Best features :', X.columns[rfecv.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of features VS. cross-validation scores\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score of number of selected features\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prior to authors:\n",
    "DescribeResult(nobs=10, minmax=(0.7092423428264374, 0.7505859928392963), mean=0.7330286516063008, variance=0.0002449243278408503, skewness=-0.16892931758355367, kurtosis=-1.5003847605685021)\n",
    "\n",
    "after some basic graphs:\n",
    "DescribeResult(nobs=10, minmax=(0.9537111539570966, 0.9556853523477206), mean=0.9544708719147975, variance=4.3393884483164826e-07, skewness=0.7947367347642024, kurtosis=-0.6317507457312379)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XG Boost\n",
    "\n",
    "1.1 XGboost base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# making sure the test and the train files have same sequence of columns\n",
    "\n",
    "test = test[X.columns]\n",
    "\n",
    "\n",
    "# defining the base model\n",
    "xgb_model_base = XGBClassifier(n_estimators = 100)\n",
    "\n",
    "# printing the cross validation scores for the classifier\n",
    "scores = cross_validate(xgb_model_base, X, y.values.ravel(), scoring='f1', \n",
    "                        cv=3,n_jobs = -1 ) # n_jobs is the number of cpus to use -1 => all\n",
    "scores\n",
    "\n",
    "\n",
    "# fitting on the training data\n",
    "xgb_model_base.fit(X, y.values.ravel())\n",
    "\n",
    "# predicting the outcome from the final \n",
    "predictions = xgb_model_base.predict(test)\n",
    "\n",
    "# write out\n",
    "out_df = test_set.copy()\n",
    "data = {'id': list(out_df.index), 'category': predictions}\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# 3: write file out\n",
    "final_df.to_csv('submission.csv',index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 XgBosst with random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the search grid\n",
    "\n",
    "random_grid = {\n",
    "     \"n_estimators\"     : [int(x) for x in np.linspace(50, 600, num = 20)],\n",
    "     \"learning_rate\"    : [0.01, 0.02, 0.05, 0.10 ] ,\n",
    "     \"max_depth\"        : [ 6, 8, 10, 12, 15, 20],\n",
    "     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "     \"gamma\"            : [ 0.3, 0.4, 0.7, 0.9 ],\n",
    "     \"colsample_bytree\" : [ 0.05, 0.1, 0.3, 0.4] }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "# First create the base model to tune\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Random search of parameters\n",
    "xgb_random = RandomizedSearchCV(estimator = xgb_model, param_distributions = random_grid,\n",
    "n_iter = 10, cv = 3, verbose=2, random_state=42 ,n_jobs = -1, scoring = 'f1_weighted')\n",
    "\n",
    "optimised_xgb_random = xgb_random.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# printing the cross validation scores for the classifier\n",
    "scores = cross_validate(optimised_xgb_random, X, y.values.ravel(), scoring='f1', \n",
    "                        cv=3,n_jobs = -1 ) # n_jobs is the number of cpus to use -1 => all\n",
    "scores\n",
    "\n",
    "\n",
    "# fitting on the training data\n",
    "xgb_model_base.fit(X, y.values.ravel())\n",
    "\n",
    "# predicting the outcome from the final \n",
    "optimised_xgb_random.predict(test)\n",
    "\n",
    "# write out\n",
    "out_df = test_set.copy()\n",
    "data = {'id': list(out_df.index), 'category': predictions}\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# 3: write file out\n",
    "final_df.to_csv('submission.csv',index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# SVM has a zero tolerance towards null values, hence replacing them by 0\n",
    "\n",
    "XVM = X.fillna(value=0)\n",
    "test_SVM = test.fillna(value=0)\n",
    "\n",
    "clf = LinearSVC( tol=1e-4)\n",
    "\n",
    "\n",
    "# printing the cross validation scores for the classifier\n",
    "scores = cross_validate(clf, XVM, y, scoring='f1', \n",
    "                        cv=10,n_jobs = -1 ) # n_jobs is the number of cpus to use -1 => all\n",
    "scores\n",
    "\n",
    "\n",
    "# fitting on the training data\n",
    "clf.fit(XVM, y)\n",
    "\n",
    "# predicting the outcome from the final \n",
    "prediction_clf = clf.predict(test_SVM)\n",
    "\n",
    "# write out\n",
    "out_df = test_set.copy()\n",
    "data = {'id': list(out_df.index), 'category': predictions}\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# 3: write file out\n",
    "final_df.to_csv('submission.csv',index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# 1: retrain the complete model -> don't forget to change this to optimal one @ end\n",
    "final_model = RandomForestClassifier()\n",
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: predict on the test set\n",
    "final_test_set = test_set.loc[:, (test_set.columns != 'source_authors') & (test_set.columns != 'common_authors') & (test_set.columns != 'target_authors')& (test_set.columns != 'label')& (test_set.columns != 'source_journal') & (test_set.columns != 'target_journal')]\n",
    "predictions = final_model.predict(final_test_set)\n",
    "\n",
    "# write out\n",
    "out_df = test_set.copy()\n",
    "data = {'id': list(out_df.index), 'category': predictions}\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# 3: write file out\n",
    "final_df.to_csv('submission.csv',index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importance\n",
    "feat_importances = pd.Series(final_model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
